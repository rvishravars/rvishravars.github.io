<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Machine Learning | World of Vishravars</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Machine Learning" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/machine-learning.html" />
<meta property="og:url" content="http://localhost:4000/machine-learning.html" />
<meta property="og:site_name" content="World of Vishravars" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Machine Learning" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","headline":"Machine Learning","url":"http://localhost:4000/machine-learning.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="World of Vishravars" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">World of Vishravars</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/software/">Software</a><a class="page-link" href="/western-classical-music/">Western classical music</a><a class="page-link" href="/machine-learning.html">Machine Learning</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Machine Learning</h1>
  </header>

  <div class="post-content">
    <h1 id="machine-learning">Machine Learning</h1>

<p><a href="/">‚Üê Back to Home</a></p>

<h2 id="-clustering-i--ii">üß© Clustering I &amp; II</h2>

<table>
  <thead>
    <tr>
      <th><strong>Term</strong></th>
      <th><strong>One-Line Explanation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Clustering</strong></td>
      <td>Grouping data points so that those within a cluster are more similar to each other than to those in other clusters.</td>
    </tr>
    <tr>
      <td><strong>K-means</strong></td>
      <td>A partitioning algorithm that iteratively assigns points to the nearest cluster mean and updates the means until convergence.</td>
    </tr>
    <tr>
      <td><strong>k-means++</strong></td>
      <td>A smart initialization method for k-means that spreads out initial cluster centers for better results.</td>
    </tr>
    <tr>
      <td><strong>Fuzzy c-means</strong></td>
      <td>A clustering method allowing data points to have fractional membership in multiple clusters.</td>
    </tr>
    <tr>
      <td><strong>Expectation-Maximization (EM)</strong></td>
      <td>Alternates between assigning data points to clusters (E-step) and updating cluster parameters (M-step) to maximize likelihood.</td>
    </tr>
    <tr>
      <td><strong>Gaussian Mixture Model (GMM)</strong></td>
      <td>Represents data as a mixture of multiple Gaussian distributions learned via EM.</td>
    </tr>
    <tr>
      <td><strong>DBSCAN</strong></td>
      <td>A density-based clustering algorithm that groups closely packed points and marks outliers as noise.</td>
    </tr>
    <tr>
      <td><strong>Core Point</strong></td>
      <td>A point with at least <em>MinPts</em> neighbors within distance <em>Eps</em>, forming the dense core of a cluster.</td>
    </tr>
    <tr>
      <td><strong>Hierarchical Clustering</strong></td>
      <td>Builds nested clusters using either a bottom-up (agglomerative) or top-down (divisive) strategy.</td>
    </tr>
    <tr>
      <td><strong>Agglomerative Clustering</strong></td>
      <td>Starts with each point as its own cluster and merges them iteratively based on distance.</td>
    </tr>
  </tbody>
</table>

<h2 id="-online-clustering-i--ii">üîÅ Online Clustering I &amp; II</h2>

<table>
  <thead>
    <tr>
      <th><strong>Term</strong></th>
      <th><strong>One-Line Explanation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Online Learning</strong></td>
      <td>Processes data incrementally as it arrives, adapting models continuously.</td>
    </tr>
    <tr>
      <td><strong>Online Averaging</strong></td>
      <td>Updates the mean iteratively with each new data point using a learning rate Œ≥‚Çô.</td>
    </tr>
    <tr>
      <td><strong>Competitive Learning (CL)</strong></td>
      <td>Neurons compete to represent input data; only the winner updates its weight.</td>
    </tr>
    <tr>
      <td><strong>Self-Organizing Map (SOM)</strong></td>
      <td>Adds a topological structure so nearby neurons update together, forming organized feature maps.</td>
    </tr>
    <tr>
      <td><strong>Neural Gas</strong></td>
      <td>Removes SOM‚Äôs grid topology and ranks neurons by distance to ensure fair updates.</td>
    </tr>
    <tr>
      <td><strong>Leader-Follower Algorithm</strong></td>
      <td>Creates new clusters when incoming data exceed a distance threshold from all existing cluster centers.</td>
    </tr>
    <tr>
      <td><strong>Change Detection</strong></td>
      <td>Identifies anomalies when new data fall outside a statistical ‚Äúthree-sigma‚Äù range.</td>
    </tr>
  </tbody>
</table>

<h2 id="-dimension-reduction-i--ii">üìâ Dimension Reduction I &amp; II</h2>

<table>
  <thead>
    <tr>
      <th><strong>Term</strong></th>
      <th><strong>One-Line Explanation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Principal Component Analysis (PCA)</strong></td>
      <td>Projects data to directions (principal components) that maximize variance.</td>
    </tr>
    <tr>
      <td><strong>Eigenvector</strong></td>
      <td>The direction of maximum variance in PCA, representing a principal component.</td>
    </tr>
    <tr>
      <td><strong>Eigenvalue</strong></td>
      <td>Measures how much variance is captured by each eigenvector.</td>
    </tr>
    <tr>
      <td><strong>Hebbian Learning</strong></td>
      <td>Neural learning principle stating that ‚Äúneurons that fire together wire together.‚Äù</td>
    </tr>
    <tr>
      <td><strong>Oja‚Äôs Rule</strong></td>
      <td>An online learning rule for neural PCA using Hebbian learning.</td>
    </tr>
    <tr>
      <td><strong>Kernel Trick</strong></td>
      <td>Computes dot products in a high-dimensional ‚Äúfeature‚Äù space without explicitly performing the transformation.</td>
    </tr>
    <tr>
      <td><strong>Kernel PCA</strong></td>
      <td>Extends PCA into a nonlinear feature space using kernel functions.</td>
    </tr>
    <tr>
      <td><strong>Multi-Dimensional Scaling (MDS)</strong></td>
      <td>Reduces dimensions by preserving pairwise distances between points.</td>
    </tr>
    <tr>
      <td><strong>Sammon‚Äôs Mapping</strong></td>
      <td>A nonlinear MDS method that minimizes distance distortion between original and reduced spaces.</td>
    </tr>
    <tr>
      <td><strong>Isomap</strong></td>
      <td>Uses graph geodesic distances to preserve manifold structure during dimension reduction.</td>
    </tr>
  </tbody>
</table>

<h2 id="-classification-iiv">üß† Classification I‚ÄìIV</h2>

<table>
  <thead>
    <tr>
      <th><strong>Term</strong></th>
      <th><strong>One-Line Explanation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Supervised Learning</strong></td>
      <td>Uses labeled data to train a model that predicts outcomes for unseen data.</td>
    </tr>
    <tr>
      <td><strong>Unsupervised Learning</strong></td>
      <td>Finds patterns in unlabeled data such as clusters or latent features.</td>
    </tr>
    <tr>
      <td><strong>Bayes‚Äô Theorem</strong></td>
      <td>Updates probability estimates based on new evidence.</td>
    </tr>
    <tr>
      <td><strong>Na√Øve Bayes Classifier</strong></td>
      <td>Assumes feature independence and applies Bayes‚Äô theorem for fast classification.</td>
    </tr>
    <tr>
      <td><strong>k-Nearest Neighbour (k-NN)</strong></td>
      <td>Classifies data points based on the majority class among their k nearest neighbors.</td>
    </tr>
    <tr>
      <td><strong>Linear Discriminant Analysis (LDA)</strong></td>
      <td>Projects data to maximize class separability using linear combinations of features.</td>
    </tr>
    <tr>
      <td><strong>Learning Vector Quantization (LVQ)</strong></td>
      <td>A supervised version of vector quantization that tunes prototypes using labeled data.</td>
    </tr>
    <tr>
      <td><strong>Radial Basis Function (RBF) Network</strong></td>
      <td>A neural model using Gaussian functions as hidden neurons to interpolate or classify data.</td>
    </tr>
    <tr>
      <td><strong>Support Vector Machine (SVM)</strong></td>
      <td>Finds an optimal hyperplane that maximizes the margin between classes.</td>
    </tr>
    <tr>
      <td><strong>Decision Tree</strong></td>
      <td>A tree-structured model that splits data recursively to reduce uncertainty or impurity.</td>
    </tr>
    <tr>
      <td><strong>Entropy</strong></td>
      <td>A measure of uncertainty or impurity in a dataset.</td>
    </tr>
    <tr>
      <td><strong>Gini Index</strong></td>
      <td>A simpler impurity measure used in decision tree algorithms like CART.</td>
    </tr>
    <tr>
      <td><strong>Feature Importance</strong></td>
      <td>Quantifies each feature‚Äôs contribution to reducing impurity in a decision tree.</td>
    </tr>
  </tbody>
</table>

<h2 id="feature-selection">Feature Selection</h2>

<table>
  <thead>
    <tr>
      <th><strong>Term</strong></th>
      <th><strong>One-line Explanation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Feature Selection</td>
      <td>Process of choosing a subset of relevant features to improve model performance and interpretability.</td>
    </tr>
    <tr>
      <td>Filter Method</td>
      <td>Ranks features using statistical measures like correlation or mutual information.</td>
    </tr>
    <tr>
      <td>Wrapper Method</td>
      <td>Uses a learning algorithm to evaluate and select feature subsets through search strategies.</td>
    </tr>
    <tr>
      <td>Embedded Method</td>
      <td>Performs feature selection during model training (e.g., LASSO, decision trees).</td>
    </tr>
    <tr>
      <td>Pearson Correlation</td>
      <td>Measures linear dependence between a feature and the target variable.</td>
    </tr>
    <tr>
      <td>Mutual Information</td>
      <td>Quantifies how much a feature reduces uncertainty about the target variable.</td>
    </tr>
    <tr>
      <td>Boruta Algorithm</td>
      <td>Random forest-based method comparing real vs. shuffled ‚Äúshadow‚Äù features.</td>
    </tr>
    <tr>
      <td>SHAP Value</td>
      <td>Game-theoretic approach to measure feature importance via contribution to predictions.</td>
    </tr>
    <tr>
      <td>mRMR</td>
      <td>Selects features that are maximally relevant to the target but minimally redundant.</td>
    </tr>
    <tr>
      <td>Sequential Forward Selection</td>
      <td>Adds one feature at a time that improves performance most.</td>
    </tr>
    <tr>
      <td>Sequential Backward Elimination</td>
      <td>Starts with all features, removes least useful features iteratively.</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="performance-evaluation">Performance Evaluation</h2>

<table>
  <thead>
    <tr>
      <th><strong>Term</strong></th>
      <th><strong>One-line Explanation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Confusion Matrix</td>
      <td>Table summarizing true vs predicted classifications.</td>
    </tr>
    <tr>
      <td>Accuracy</td>
      <td>Proportion of correctly classified samples.</td>
    </tr>
    <tr>
      <td>Precision</td>
      <td>Fraction of positive predictions that are correct.</td>
    </tr>
    <tr>
      <td>Recall / Sensitivity</td>
      <td>Fraction of true positives correctly identified.</td>
    </tr>
    <tr>
      <td>Specificity</td>
      <td>Fraction of true negatives correctly identified.</td>
    </tr>
    <tr>
      <td>F1 Score</td>
      <td>Harmonic mean of precision and recall.</td>
    </tr>
    <tr>
      <td>MCC</td>
      <td>Balanced metric for binary classification, effective for imbalanced data.</td>
    </tr>
    <tr>
      <td>ROC Curve</td>
      <td>Plots true positive rate vs. false positive rate at varying thresholds.</td>
    </tr>
    <tr>
      <td>AUC</td>
      <td>Area under the ROC curve; measures overall model discrimination ability.</td>
    </tr>
    <tr>
      <td>Cross-Validation</td>
      <td>Evaluates model performance using multiple data splits.</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="regression">Regression</h2>

<table>
  <thead>
    <tr>
      <th><strong>Term</strong></th>
      <th><strong>One-line Explanation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Regression</td>
      <td>Predicts continuous numeric outcomes from input features.</td>
    </tr>
    <tr>
      <td>Ordinary Least Squares</td>
      <td>Minimizes mean squared error to estimate model coefficients.</td>
    </tr>
    <tr>
      <td>In-sample Error</td>
      <td>Error on the training data used for fitting the model.</td>
    </tr>
    <tr>
      <td>Out-of-sample Error</td>
      <td>Error on unseen test data, measuring generalization.</td>
    </tr>
    <tr>
      <td>Linear Regression</td>
      <td>Predicts outcome as a weighted sum of input features.</td>
    </tr>
    <tr>
      <td>Polynomial Regression</td>
      <td>Extends linear regression by including polynomial terms of inputs.</td>
    </tr>
    <tr>
      <td>Gradient Descent</td>
      <td>Iterative optimization method for minimizing loss.</td>
    </tr>
    <tr>
      <td>Learning Rate</td>
      <td>Step size determining how much to adjust parameters in gradient descent.</td>
    </tr>
    <tr>
      <td>Regularization</td>
      <td>Penalizes model complexity to prevent overfitting.</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="regression-2">Regression 2</h2>

<table>
  <thead>
    <tr>
      <th><strong>Term</strong></th>
      <th><strong>One-line Explanation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Ridge Regression</td>
      <td>L2 regularization that shrinks coefficients to reduce variance.</td>
    </tr>
    <tr>
      <td>LASSO Regression</td>
      <td>L1 regularization that drives some coefficients to zero (feature selection).</td>
    </tr>
    <tr>
      <td>Decision Tree Regressor</td>
      <td>Splits data space recursively to minimize prediction error.</td>
    </tr>
    <tr>
      <td>R¬≤ (Coefficient of Determination)</td>
      <td>Measures proportion of variance explained by the model.</td>
    </tr>
    <tr>
      <td>RMSE</td>
      <td>Square root of mean squared error; penalizes large errors.</td>
    </tr>
    <tr>
      <td>MAE</td>
      <td>Mean of absolute prediction errors; less sensitive to outliers.</td>
    </tr>
    <tr>
      <td>Regularization Parameter (Œª)</td>
      <td>Controls trade-off between bias and variance.</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="ensemble-learning">Ensemble Learning</h2>

<table>
  <thead>
    <tr>
      <th><strong>Term</strong></th>
      <th><strong>One-line Explanation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Ensemble Learning</td>
      <td>Combines multiple models to improve overall prediction accuracy.</td>
    </tr>
    <tr>
      <td>Bagging</td>
      <td>Trains models on bootstrapped samples and averages predictions.</td>
    </tr>
    <tr>
      <td>Boosting</td>
      <td>Sequentially trains models to focus on previous errors.</td>
    </tr>
    <tr>
      <td>Random Forest</td>
      <td>Ensemble of decision trees using random feature and data sampling.</td>
    </tr>
    <tr>
      <td>AdaBoost</td>
      <td>Assigns higher weights to misclassified samples for next learner.</td>
    </tr>
    <tr>
      <td>Gradient Boosting</td>
      <td>Fits new models to residuals from previous models.</td>
    </tr>
    <tr>
      <td>Voting</td>
      <td>Combines predictions via majority or weighted averaging.</td>
    </tr>
    <tr>
      <td>Diversity</td>
      <td>Independence among base learners that improves ensemble performance.</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="model-selection">Model Selection</h2>

<table>
  <thead>
    <tr>
      <th><strong>Term</strong></th>
      <th><strong>One-line Explanation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Bias</td>
      <td>Systematic error due to overly simple models.</td>
    </tr>
    <tr>
      <td>Variance</td>
      <td>Sensitivity to fluctuations in training data.</td>
    </tr>
    <tr>
      <td>Bias-Variance Tradeoff</td>
      <td>Balancing underfitting (bias) vs overfitting (variance).</td>
    </tr>
    <tr>
      <td>Cross-Validation</td>
      <td>Estimates generalization by testing on unseen folds.</td>
    </tr>
    <tr>
      <td>Regularization</td>
      <td>Adds penalty terms to control model complexity.</td>
    </tr>
    <tr>
      <td>AIC</td>
      <td>Akaike Information Criterion for model comparison using likelihood and parameters.</td>
    </tr>
    <tr>
      <td>BIC</td>
      <td>Bayesian Information Criterion penalizing model complexity more strongly.</td>
    </tr>
    <tr>
      <td>Hyperparameter Tuning</td>
      <td>Adjusting non-learnable settings (e.g., k in KNN) to optimize performance.</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="multilayer-perceptron">Multilayer Perceptron</h2>

<table>
  <thead>
    <tr>
      <th><strong>Term</strong></th>
      <th><strong>One-line Explanation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Perceptron</td>
      <td>Basic linear classifier using weighted sums and activation.</td>
    </tr>
    <tr>
      <td>Activation Function</td>
      <td>Nonlinear function applied to neuron outputs (e.g., ReLU, sigmoid).</td>
    </tr>
    <tr>
      <td>Backpropagation</td>
      <td>Algorithm to compute gradients and update weights in neural networks.</td>
    </tr>
    <tr>
      <td>Learning Rate</td>
      <td>Determines how fast weights are updated during training.</td>
    </tr>
    <tr>
      <td>Momentum</td>
      <td>Adds a fraction of previous weight update to speed convergence.</td>
    </tr>
    <tr>
      <td>Vanishing Gradient</td>
      <td>Gradients shrink in deep networks, hindering learning.</td>
    </tr>
    <tr>
      <td>Mini-batch Gradient Descent</td>
      <td>Uses small data subsets for faster, smoother updates.</td>
    </tr>
    <tr>
      <td>ADAM</td>
      <td>Adaptive optimization combining momentum and RMSprop ideas.</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="convolutional-neural-networks">Convolutional Neural Networks</h2>

<table>
  <thead>
    <tr>
      <th><strong>Term</strong></th>
      <th><strong>One-line Explanation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Convolution</td>
      <td>Operation applying filters over input data to extract features.</td>
    </tr>
    <tr>
      <td>Pooling</td>
      <td>Reduces spatial size while retaining important information.</td>
    </tr>
    <tr>
      <td>LeNet-5</td>
      <td>Early CNN architecture for digit recognition.</td>
    </tr>
    <tr>
      <td>AlexNet</td>
      <td>Deep CNN that popularized GPU training and ReLU activations.</td>
    </tr>
    <tr>
      <td>YOLO</td>
      <td>Real-time object detection architecture (‚ÄúYou Only Look Once‚Äù).</td>
    </tr>
    <tr>
      <td>ResNet</td>
      <td>Deep CNN using residual (skip) connections to ease training.</td>
    </tr>
    <tr>
      <td>Vision Transformer</td>
      <td>Applies attention mechanisms from NLP to image recognition.</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="variational-autoencoder-vae">Variational Autoencoder (VAE)</h2>

<table>
  <thead>
    <tr>
      <th><strong>Term</strong></th>
      <th><strong>One-line Explanation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Autoencoder</td>
      <td>Neural network that reconstructs inputs from compressed latent space.</td>
    </tr>
    <tr>
      <td>Variational Autoencoder</td>
      <td>Probabilistic autoencoder learning a smooth latent distribution.</td>
    </tr>
    <tr>
      <td>Encoder</td>
      <td>Maps input data to latent variables.</td>
    </tr>
    <tr>
      <td>Decoder</td>
      <td>Reconstructs data from latent variables.</td>
    </tr>
    <tr>
      <td>Latent Space</td>
      <td>Compressed representation capturing essential features.</td>
    </tr>
    <tr>
      <td>KL Divergence</td>
      <td>Measures difference between two probability distributions.</td>
    </tr>
    <tr>
      <td>Reparameterization Trick</td>
      <td>Enables gradient-based learning through stochastic sampling.</td>
    </tr>
    <tr>
      <td>ELBO</td>
      <td>Evidence Lower Bound, objective function optimized in VAEs.</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="generative-adversarial-network-gan">Generative Adversarial Network (GAN)</h2>

<table>
  <thead>
    <tr>
      <th><strong>Term</strong></th>
      <th><strong>One-line Explanation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>GAN</td>
      <td>Framework with generator and discriminator competing in a minimax game.</td>
    </tr>
    <tr>
      <td>Generator</td>
      <td>Produces synthetic data from random noise.</td>
    </tr>
    <tr>
      <td>Discriminator</td>
      <td>Distinguishes real data from generated data.</td>
    </tr>
    <tr>
      <td>Minimax Game</td>
      <td>Optimization framework where G tries to fool D and D tries to detect fakes.</td>
    </tr>
    <tr>
      <td>Mode Collapse</td>
      <td>Failure where generator produces limited data diversity.</td>
    </tr>
    <tr>
      <td>Nash Equilibrium</td>
      <td>State where neither G nor D can improve performance unilaterally.</td>
    </tr>
    <tr>
      <td>CycleGAN</td>
      <td>Uses cyclic consistency for image-to-image translation without paired data.</td>
    </tr>
    <tr>
      <td>Super-Resolution GAN</td>
      <td>Enhances image resolution using adversarial and perceptual loss.</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="transfer-learning-1">Transfer Learning 1</h2>

<table>
  <thead>
    <tr>
      <th><strong>Term</strong></th>
      <th><strong>One-line Explanation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Transfer Learning (TL)</strong></td>
      <td>Applying knowledge learned in one domain/task to another related domain/task.</td>
    </tr>
    <tr>
      <td><strong>Source Domain</strong></td>
      <td>The domain providing prior knowledge or training data.</td>
    </tr>
    <tr>
      <td><strong>Target Domain</strong></td>
      <td>The new domain where learned knowledge is applied.</td>
    </tr>
    <tr>
      <td><strong>Inductive Transfer Learning</strong></td>
      <td>TL where labeled data exists in the target domain.</td>
    </tr>
    <tr>
      <td><strong>Transductive Transfer Learning</strong></td>
      <td>TL with labeled data only in the source domain.</td>
    </tr>
    <tr>
      <td><strong>Unsupervised Transfer Learning</strong></td>
      <td>TL where both domains lack labeled data.</td>
    </tr>
    <tr>
      <td><strong>Self-taught Learning</strong></td>
      <td>Learning features from unlabeled data and transferring them to labeled tasks.</td>
    </tr>
    <tr>
      <td><strong>Domain Adaptation</strong></td>
      <td>Adapting models from a source domain to an unlabeled target domain.</td>
    </tr>
    <tr>
      <td><strong>Instance-based Transfer</strong></td>
      <td>Re-weighting instances from the source domain to reduce domain bias.</td>
    </tr>
    <tr>
      <td><strong>TrAdaBoost</strong></td>
      <td>Boosting-based TL algorithm that adjusts instance weights between domains.</td>
    </tr>
    <tr>
      <td><strong>Sparse Coding</strong></td>
      <td>Learning sparse feature representations to transfer across domains.</td>
    </tr>
    <tr>
      <td><strong>Hierarchical Transfer</strong></td>
      <td>Multi-level knowledge transfer to handle complex or multi-task learning.</td>
    </tr>
    <tr>
      <td><strong>Covariate Shift</strong></td>
      <td>Situation where input distributions differ between training and testing domains.</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="deep-transfer-learning">Deep Transfer Learning</h2>

<table>
  <thead>
    <tr>
      <th><strong>Term</strong></th>
      <th><strong>One-line Explanation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Deep Transfer Learning (DTL)</strong></td>
      <td>Using deep neural networks to transfer features or representations between domains.</td>
    </tr>
    <tr>
      <td><strong>Domain Invariance</strong></td>
      <td>Ensuring learned features are independent of domain differences.</td>
    </tr>
    <tr>
      <td><strong>Adversarial Domain Adaptation (DANN)</strong></td>
      <td>Using adversarial training to make feature representations domain-invariant.</td>
    </tr>
    <tr>
      <td><strong>VAE (Variational Autoencoder)</strong></td>
      <td>A generative model used to learn continuous latent representations.</td>
    </tr>
    <tr>
      <td><strong>GAN (Generative Adversarial Network)</strong></td>
      <td>A generative model where two networks compete to produce realistic outputs.</td>
    </tr>
    <tr>
      <td><strong>UNIT (Unsupervised Image-to-Image Translation)</strong></td>
      <td>Framework combining VAE and GAN to translate images across domains.</td>
    </tr>
    <tr>
      <td><strong>CGGS/DATL</strong></td>
      <td>Cross-Grafted Generative Stacks / Deep Adversarial Transfer Learning; creates transition domains.</td>
    </tr>
    <tr>
      <td><strong>Latent Space Mixup</strong></td>
      <td>Mixing latent representations to create hybrid samples for domain generalization.</td>
    </tr>
    <tr>
      <td><strong>CDLM (Cross-Domain Latent Modulation)</strong></td>
      <td>A method to modulate latent variables for bi-directional domain adaptation.</td>
    </tr>
    <tr>
      <td><strong>Taskonomy</strong></td>
      <td>Study of task relationships to learn how tasks transfer knowledge efficiently.</td>
    </tr>
    <tr>
      <td><strong>t-SNE Visualization</strong></td>
      <td>A technique for visualizing high-dimensional domain representations.</td>
    </tr>
    <tr>
      <td><strong>Affinity Matrix</strong></td>
      <td>Measures transferability among tasks based on learned representations.</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="federated-learning">Federated Learning</h2>

<table>
  <thead>
    <tr>
      <th><strong>Term</strong></th>
      <th><strong>One-line Explanation</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Federated Learning (FL)</strong></td>
      <td>Collaborative training of models across multiple devices without sharing raw data.</td>
    </tr>
    <tr>
      <td><strong>FedAvg</strong></td>
      <td>A core FL algorithm that averages locally trained model weights across clients.</td>
    </tr>
    <tr>
      <td><strong>Split Learning</strong></td>
      <td>Dividing models between clients and servers to preserve data privacy.</td>
    </tr>
    <tr>
      <td><strong>Data Heterogeneity</strong></td>
      <td>Variation in local datasets across clients (non-IID data).</td>
    </tr>
    <tr>
      <td><strong>Resource Heterogeneity</strong></td>
      <td>Differences in client computational and communication capabilities.</td>
    </tr>
    <tr>
      <td><strong>Hetero-FL</strong></td>
      <td>Federated setup allowing clients to use different model architectures.</td>
    </tr>
    <tr>
      <td><strong>FedMD</strong></td>
      <td>FL framework using model distillation and transfer learning for heterogeneous clients.</td>
    </tr>
    <tr>
      <td><strong>CloREF</strong></td>
      <td>Rule-based collaborative FL method allowing different model types (e.g., RF, SVM).</td>
    </tr>
    <tr>
      <td><strong>Gradient Inversion Attack</strong></td>
      <td>Technique to reconstruct private data from shared gradients.</td>
    </tr>
    <tr>
      <td><strong>Secure Aggregation</strong></td>
      <td>Cryptographic protocol to protect individual client updates during aggregation.</td>
    </tr>
    <tr>
      <td><strong>LOKI Attack</strong></td>
      <td>Advanced data leakage attack that reconstructs data even in secure aggregation.</td>
    </tr>
    <tr>
      <td><strong>Flower Framework</strong></td>
      <td>Open-source framework for building and simulating federated learning systems.</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="references">References</h2>

<ul>
  <li>Pan &amp; Yang, <em>A Survey on Transfer Learning</em>, IEEE TKDE 2010.</li>
  <li>Raina et al., <em>Self-taught Learning</em>, ICML 2007.</li>
  <li>Ganin &amp; Lempitsky, <em>Unsupervised Domain Adaptation by Backpropagation (DANN)</em>, ICML 2015.</li>
  <li>Liu et al., <em>UNIT: Unsupervised Image-to-Image Translation Networks</em>, NeurIPS 2017.</li>
  <li>Xu et al., <em>Adversarial Domain Adaptation with Domain Mixup</em>, AAAI 2020.</li>
  <li>Hou et al., <em>Cross-Domain Latent Modulation</em>, WACV 2021.</li>
  <li>McMahan et al., <em>Communication-efficient Learning of Deep Networks from Decentralized Data</em>, AISTATS 2017.</li>
  <li>Bonawitz et al., <em>Practical Secure Aggregation for Federated Learning</em>, NIPS 2016.</li>
  <li>Pang et al., <em>Rule-based Collaborative Learning (CloREF)</em>, PAKDD 2022 / IEEE TKDE 2023.</li>
</ul>

<hr />

<p><a href="#machine-learning">‚Üë Back to Top</a></p>

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">World of Vishravars</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">World of Vishravars</li><li><a class="u-email" href="mailto:vishravars@gmail.com">vishravars@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/rvishravars"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">rvishravars</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
